from segment import Segment
import numpy as np
import altair as alt
import pandas as pd

alt.data_transformers.disable_max_rows()


def policy(direction):
    """
    Determines the next action to take

    Parameters:
        direction (str): right, left, or random

    Returns:
        (int): the direction to follow as an integer
    """
    if direction not in ("right", "left", "random"):
        raise ValueError("direction must be right, left, or random")

    if direction == "right":
        return 1
    elif direction == "left":
        return 0
    else:
        return np.random.randint(0, 2)
    

def run_episode(num_actions=2, start_observation=10, p=0.8, terminal=10, 
                direction="right"):
    """
    Run a single episode in the Segment environment

    Parameters:
        num_actions (int): the number of possible actions
        start_observation (int): the starting location
        p (float): the probability that the agent takes the intended action
        terminal (int): half of the absolute value of the terminal states
        e (int): the current episode

    Returns:
        tuple (int, int, float, int): observed states, actions, rewards, time
    """
    t, tMAX = 0, 50
    done = False
    observation = start_observation
    observations = [observation]
    rewards, actions = [], []
    env = Segment(num_actions, start_observation, p, terminal, done)

    while t < tMAX and done == False:
        action = policy(direction=direction)
        observation_next, reward, done = env.step(action=action)
        observations.append(observation_next)
        rewards.append(reward); actions.append(action)
        observation = observation_next
        t+=1
        
    dta = pd.DataFrame(
             [range(0, t),
              observations[0:t],
              observations[1:(t + 1)],
              actions[0:t],
              rewards]).transpose()

    dta.columns = ['t','observation', 'observation_next','action','reward']

    dta['t'] = dta.t.astype(int)
    dta['observation'] = dta['observation'].astype(int) - 10
    dta['observation_next'] = dta['observation_next'].astype(int) - 10
    dta['beta'] = 0.98
    dta['beta^t'] = dta['beta']**dta['t']
    dta['beta^t_reward'] = dta['beta^t']*dta['reward']

    return dta


def run_episodes(num_episodes=1000, num_actions=2, start_observation=10, p=0.8, 
                 terminal=10, direction="right"):
    """
    Run multiple episodes in the Segment environment

    Parameters:
        num_episodes (int): the number of episodes to run
        num_actions (int): the number of possible actions
        start_observation (int): the starting location
        p (float): the probability that the agent takes the intended action
        terminal (int): half of the absolute value of the terminal states

    Returns:
        tuple (int, int, float, int): observed states, actions, rewards, time
    """
    df = pd.DataFrame()

    for ep in range(num_episodes):
        current_df = run_episode(num_actions, start_observation, 
                                 p, terminal, direction)
        current_df['episode'] = ep
        df = pd.concat([df, current_df])

    return df


def estimate_v(num_episodes=1000, num_actions=2, start_observation=10, p=0.8, 
               terminal=10, direction="right"):
    """
    Estimate the values of starting observations in the Segment environment

    Parameters:
        num_actions (int): the number of possible actions
        start_observation (int): the starting location
        p (float): the probability that the agent takes the intended action
        terminal (int): half of the absolute value of the terminal states

    Returns:
        tuple (int, int, float): observed states, actions, rewards
    """
    df = pd.DataFrame()
    obs = range(1, 20)

    for o in obs:
        for ep in range(num_episodes):
            current_obs = run_episode(num_actions, o, 
                                 p, terminal, direction)
            current_obs['initial_state'] = o - 10
            current_obs['episode'] = ep
            df = pd.concat([df, current_obs])


    return df


def summarize(df, num_episodes=1000):
    """
    Summarize the results of estimating the values of all states

    Parameters:
        df (pd.DataFrame): a dtaframe generated by estimate_v
        num_episodes (int): the number of episodes that were run

    Returns:
        (pd.DataFrame): a summary of the state values
    """
    df =  df.groupby('initial_state')['beta^t_reward'].describe(
            percentiles=[0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]).reset_index()
    # Replace count with the number of episodes
    df['count'] = pd.Series([num_episodes] * df.shape[0]).astype(int)

    return df


def tab_2_4(df):
    """
    Recreates table 2.4

    Parameters:
        data (pd.DataFrame): a dataframe generated by run_episodes

    Returns:
        (pd.DataFrame): the data from table 2.4
    """
    return pd.DataFrame(df.groupby('episode')['beta^t_reward'].mean().describe(
        percentiles=[0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]))


def plot_fig_2_2(data):
    """
    Recreates the plot in figure 2.2

    Parameters:
        data (pd.DataFrame): a dataframe generated by run_episodes

    Returns:
        (alt.Chart): an altair chart
    """
    df = pd.DataFrame(data.groupby("episode")["beta^t_reward"].agg("sum"))
    return alt.Chart(df, title="Rewards accross all episodes").mark_bar().encode(
        x=alt.X("beta^t_reward:Q",
              bin=True,
              title="Discounted sum of rewards"),
        y=alt.Y('count()',
                title="Count")
        )


def plot_fig_2_3(data):
    """
    Recreates the plot in figure 2.3

    Parameters:
        data (pd.DataFrame): a dataframe generated by estimate_v

    Returns:
        (alt.Chart): an altair chart
    """
    data = data.groupby(["initial_state", 
                         "episode"])["beta^t_reward"].sum().reset_index().groupby(
        ["initial_state"])["beta^t_reward"].mean().to_frame().reset_index()

    return alt.Chart(data, title="Sum of discounted rewards for each state").mark_circle(
        color="maroon").encode(
        x=alt.X('initial_state',
                title="Initial state, s0"),
        y=alt.Y('beta^t_reward',
                title='Value function V(s0)')
    )


def transition_matrix(df):
    """
    Create a transition matrix from all states and episodes

    Parameters:
        df (pd.DataFrame): a dataframe from calling estimate_v
    """
    t = df.groupby(["observation", "observation_next"], as_index=False).size()
    t['prob'] = t['size'] / t.groupby('observation')['size'].transform('sum')
    t = pd.pivot_table(t, index=['observation'], columns=['observation_next'], 
                       values='prob', fill_value=0)

    return t.style.background_gradient()

