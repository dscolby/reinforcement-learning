{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Darren Colby\n",
    "\n",
    "MSCA 32020\n",
    "\n",
    "Value Iteration\n",
    "\n",
    "The purpose of this notebook is to get an intuiton for how value iteration works by implementing it from scratch and use the implementation to solve a simple grid problem. Towards that purpose, I have tried to make this implementation as clear as possible by using dictionaries instead of matrices, using action names like \"Right\" instead of an arbitrary number or letter, and labeling variables with names like transition_probabilities instead of T or P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "using StatsBase\n",
    "using LinearAlgebra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define an MDP Struct\n",
    "\n",
    "To make our lives easier we will make a struct to store all the components for a simple MDP. We can initialize an MDP by passing a tuple of states, a tuple of actions, a dictionary of rewards, and a dictionary of transition probabilities or dictionary of dictionaries of transition probabilities if we want our transition probabilities to depend on the action the agent takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDP"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Struct to store the necessary componeents of a Markov Decision Process\"\"\"\n",
    "struct MDP\n",
    "    states::Tuple\n",
    "    actions::Tuple\n",
    "    rewards::Dict\n",
    "    transition_probabilities::Dict\n",
    "    num_states::Int64\n",
    "    num_actions::Int64\n",
    "    γ::Float64\n",
    "    values::Dict\n",
    "    π::Vector{Float64}\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    MDP(states, actions, rewards, transitiion_probabilities)\n",
    "\n",
    "    Create an MDP struct to solve\n",
    "\"\"\"\n",
    "function MDP(states::Tuple, actions::Tuple, rewards::Dict, transition_probabilities::Dict, \n",
    "    γ::Float64=0.9)\n",
    "\n",
    "    # Initialize values to zero    \n",
    "    values = Dict(state => 0.0 for state in states)\n",
    "\n",
    "        MDP(states, actions, rewards, transition_probabilities, size(states, 1), \n",
    "            size(actions, 1), γ, values, zeros(size(states, 1)))\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Update Values for Each Action\n",
    "\n",
    "To solve our MDP with value iteration we will define functions inside out. In other words, we will start by defining a function to update the values for a single action using the Bellman Optimality Condition. Then, we can call this functiion for each action and update its value. To do this we will get our next action (s') by using our current state and action to sample from the transition probabilities. Then, we will update the value of our action by adding its immediate reward r(s, a) from the rewards dictionary to the discounted sum of the product of our transition probabilities P(s'|s, a) multiplied by each possible next state value (s') in our values dictionary. This update will be of the form $V(a) = r(s, a) + \\gamma{\\Sigma{P(s'|s, a)*V(a')}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bellmanupdate"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "gettransitionprobability(mdp, state, action)\n",
    "\n",
    "Get the next state and its transition probability\n",
    "\"\"\"\n",
    "function gettransitionprobability(mdp::MDP, state, action)\n",
    "    # Get the next state based on its probability given the current state and action\n",
    "    possible_states = collect(keys(mdp.transition_probabilities[action][state]))\n",
    "    state_probs = ProbabilityWeights(collect(values(mdp.transition_probabilities[action][state])))\n",
    "    next_state = sample(possible_states, state_probs)\n",
    "\n",
    "    return mdp.transition_probabilities[action][state][next_state], next_state\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "bellmanupdate(mdp, action_values, state, action)\n",
    "\n",
    "Update the estimated value of an action at a given state using the Bellman Optimality Condition\n",
    "\"\"\"\n",
    "function bellmanupdate(mdp::MDP, action_values::Dict, state, action)\n",
    "\n",
    "    # Get transition probabilities from the given state and action\n",
    "    transition_probabilities = values(mdp.transition_probabilities[action][state])\n",
    "\n",
    "    # Get the immediate reward for that state\n",
    "    reward = mdp.rewards[state][action]\n",
    "\n",
    "    # Immediate reward plus discounted expectation of values for next possible states\n",
    "    action_values[action] = float(reward) + (mdp.γ*dot(transition_probabilities, values(mdp.values)))\n",
    "    \n",
    "    return action_values\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Estimate the Values for Each State\n",
    "\n",
    "The value for a state under an optimal policy is $r(s, a) + \\gamma max_a{\\Sigma{P(s'|s, a)*V(a')}}.$  This is what the function above does, only, it does not do the max operation. To estimate the value of our states we iterate through each state, iterate through each action using the function above, and assign the value of a state as the value of the action with the highest expected discounted reward that the agent can take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estimateV!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "estimateV!(mdp)\n",
    "\n",
    "Estimate the value of a state\n",
    "\"\"\"\n",
    "function estimateV!(mdp::MDP)\n",
    "    Δ = Vector{Float64}(undef, mdp.num_states)\n",
    "    for (i, state) in enumerate(mdp.states)\n",
    "        temp = deepcopy(mdp.values[state]) \n",
    "        action_values = Dict(action => 0.0 for action in mdp.actions)\n",
    "        \n",
    "        # Collect the rewards for taking each action at each state\n",
    "        for action in mdp.actions\n",
    "\n",
    "            # If the agent hits the absorbing state\n",
    "            if gettransitionprobability(mdp, state, action) ∈ (0, 1)\n",
    "                mdp.values[state] = 0\n",
    "                break\n",
    "            else\n",
    "                # Get updated values for each action\n",
    "            action_values = bellmanupdate(mdp, action_values, state, action)\n",
    "            end\n",
    "\n",
    "        # Update the MDP value with the highest temp value caused by the best action\n",
    "        mdp.values[state] = maximum(collect(values(action_values)))\n",
    "        Δ[i] = abs(temp - mdp.values[state])\n",
    "        end\n",
    "    end\n",
    "    return maximum(Δ)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Finding the Optimal Policy\n",
    "\n",
    "Now that we have a way to estimate the optimal values for each state, we can use them to find the optimal policy, give by $\\pi^* = argmax_a\\Sigma_{s'}{P(s'|s, a)v(s')}.$ To do this we use nearly the same method we used to estimate the value function. We iterate through each state and each action, substitute our estimated state values into the equation above, use it to find the best action at each state, and map those states and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getpolicy"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "getpolicy(mdp)\n",
    "\n",
    "Estimate the optimal policy for an MDP using estimated state values\n",
    "\"\"\"\n",
    "function getpolicy(mdp::MDP)\n",
    "    policy = Dict()\n",
    "    for state in mdp.states in \n",
    "        action_values = Dict()\n",
    "        \n",
    "        for action in mdp.actions\n",
    "           action_values = bellmanupdate(mdp, action_values, state, action) \n",
    "        end\n",
    "        policy[state] = findmax(action_values)[2]\n",
    "    end\n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Putting It All Together\n",
    "\n",
    "To solve an MDP is estimate V for many iterations until the values stop changing a lot. Then, we can use the function defined above to coninually improve the estimated state values within a certain tolerance. Using these estimated state values we use the getpolicyfunction to find an optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solve!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "solve!(mdp, tolerance)\n",
    "\n",
    "Solve a Markov Decision Process using value iteration\n",
    "\"\"\"\n",
    "function solve!(mdp::MDP, tolerance::Float64=1e-5)\n",
    "    iteration = 0\n",
    "    Δ = estimateV!(mdp)\n",
    "\n",
    "    while Δ > tolerance\n",
    "        iteration += 1\n",
    "        Δ = estimateV!(mdp)\n",
    "        println(\"Change in state value: \" * string(Δ))\n",
    "\n",
    "        if Δ < tolerance\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return getpolicy(mdp), iteration\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple MDP Problem\n",
    "\n",
    "To show that the above implementation works, we can solve a very simple problem. Similar to the robot problem we have seen in class, we can imagine a segment of grid sqaures (states) 0, 1, 2, and 3. The goal of the robot is to get to square 3, which has a reward of 5. The other squares impose a fuel cost of -1. Accordingly, the robot's action space consists of the actions \"Right\" and \"Left.\" Below is also a dictionary of transition probabilities that defines the probability of starting in one state and ending in another. Like we saw in class, there can be different transition matrices for each action, as there is for the example here. One thing to notice is that this is an infinite horizon problem--a new episode does not restart when the robot reaches square 3--instead, square 3 is an absorbing state and any action the robot takes in this absorbing state has a reward of zero. Square 0 is a bit different because square 0 is not the robots goal, so if the robot stays in square 0 it can still go right towards its goal. It would also be possible to solve the full robot problem from class but that would require manually typing more states, actions, and values, which I'm too lazy to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Dict{Int64, Dict{Int64}}} with 2 entries:\n",
       "  \"Right\" => Dict(0=>Dict{Int64, Real}(0=>0.2, 2=>0, 3=>0, 1=>0.8), 2=>Dict{Int…\n",
       "  \"Left\"  => Dict(0=>Dict{Int64, Real}(0=>0.8, 2=>0, 3=>0, 1=>0.2), 2=>Dict{Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "states = Tuple(i for i in 0:3)\n",
    "actions = tuple(\"Left\", \"Right\")\n",
    "rewards = Dict(0 => Dict(\"Left\" => -1, \"Right\" => -1), \n",
    "           1 => Dict(\"Left\" => -1, \"Right\" => -1), \n",
    "           2 => Dict(\"Left\" => -1, \"Right\" => 5),\n",
    "           3 => Dict(\"Left\" => 0, \"Right\" => 0))\n",
    "transition_probabilities = Dict(\"Left\" => Dict(0 => Dict(0 => 0.8, 1 => 0.2, 2 => 0, 3 => 0), \n",
    "                                               1 => Dict(0 => 0.8, 1 => 0, 2 => 0.2, 3 => 0), \n",
    "                                               2 => Dict(0 => 0, 1 => 0.8, 2 => 0, 3 => 0.2), \n",
    "                                               3 => Dict(0 => 0, 1 => 0, 2 => 0, 3 => 1)), \n",
    "                                \"Right\" => Dict(0 => Dict(0 => 0.2, 1 => 0.8, 2 => 0, 3 => 0), \n",
    "                                                1 => Dict(0 => 0.2, 1 => 0, 2 => 0.8, 3 => 0), \n",
    "                                                2 => Dict(0 => 0, 1 => 0.2, 2 => 0, 3 => 0.8), \n",
    "                                                3 => Dict(0 => 0, 1 => 0, 2 => 0, 3 => 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in state value: 3.294144\n",
      "Change in state value: 2.3717836800000005\n",
      "Change in state value: 0.6147663298560007\n",
      "Change in state value: 0.23002953889097233\n",
      "Change in state value: 0.0825459302565994\n",
      "Change in state value: 0.02871165260908315\n",
      "Change in state value: 0.009944082240489927\n",
      "Change in state value: 0.0034420075516421456\n",
      "Change in state value: 0.0011913039277553494\n",
      "Change in state value: 0.0004123141420593335\n",
      "Change in state value: 0.00014270302312979766\n",
      "Change in state value: 4.93898851234853e-5\n",
      "Change in state value: 1.7093966337089483e-5\n",
      "Change in state value: 5.9162657106703875e-6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dict{Any, Any}(0 => \"Right\", 2 => \"Right\", 3 => \"Right\", 1 => \"Right\"), 14)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_robot_mdp = MDP(states, actions, rewards, transition_probabilities)\n",
    "solve!(simple_robot_mdp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
